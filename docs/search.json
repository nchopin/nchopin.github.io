[
  {
    "objectID": "posts/2023-06-27-test/index.html",
    "href": "posts/2023-06-27-test/index.html",
    "title": "New web-site",
    "section": "",
    "text": "Hi everyone, welcome to my new, shiny, quarto-based web-site.\nI moved my website to github a few years ago, and tried to use Jekyll, as recommended by the github pages documentation. However, Jekyll seems to be a bit overkill for a simple academic website. I tried to start from this template, but I found it difficult to adapt it even a little bit (and it’s not maintained anymore)\nBottom line: I strongly recommend quarto, it took me much less time to obtain the website I am happy with, it also looks more flexible (for my use), the documentation is great, and I even managed to add this blog (which is not something I dared to do with Jekyll).\nAlso, math support is great: \\(2+\\int_0^1 f(x) dx\\). You can also type code:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "book_typos.html",
    "href": "book_typos.html",
    "title": "Typos and errors",
    "section": "",
    "text": "A lot of thanks to anyone who found the errors below (their names appear in parentheses). Feel free to contact me if you find others.\n\nMajor mistakes\n\n100, Exercise 8.7: the fact that the former estimator dominates the latter (in terms of asymptotic variance) is equivalent to the stated equality provided \\(\\mathbb{Q}(\\varphi)&gt;0\\) (this condition is omitted in the book). Moreover, when \\(w\\) is an indicator function, one has equality (both estimators have the same variance asymptotically). (Otmane Sakhi)\n117, Algorithm 9.6: the last two instructions should be outside the while loop, not inside; i.e. while \\(v^m &lt; s\\), increment \\(m\\). And, only when the while loop has ended, we assign \\(A^n\\) and increment \\(s\\). (Matt Wand)\n171, Lemma 1.2: assumption that \\(G_t\\) is upper bounded is not required. (Adrien Corenflos)\n182, Lemma 11.10: this lemma is correct only if function \\(\\varphi\\) takes both positive and negative values. In the following calculations, this lemma is applied to centred functions (\\(\\varphi\\) minus its expectations). (Adam Johansen)\n313, Prop. 16.6 and Algorithm 16.8: the expression for the weights (which define the distribution of \\(B_t\\)) is correct for the bootstrap filter, but not in general, as a factor \\(G_{t+1}(X_t^n, X_{t+1}^{B_{t+1}})\\) is missing. (In the bootstrap filter, this factor is constant, since \\(G_{t+1}\\) does not depend on the first argument in this case.) Alternatively, you can also replace the transition density \\(m_{t+1}\\) by the one of the model, \\(p_{t+1}\\), to obtain a correct expression. (Adrien Corenflos) Also, in light of this, the sentence below Prof. 16.6 is a bit daft, and should be ignored.\n\n\n\nTypos\n\n41, second equation: missing \\(dx_{t-1}\\) in the integral (Chihiro Kuraya)\n44, first equation: \\(f_t\\) should be \\(f_s\\) (Chihiro Kuraya)\n56, first equation: \\(M_{0:t-1}(dx_{t-1})\\) should be \\(M_{t-1}(dx_{0:t-1}\\)) (Feras Saad)\n59, first equation: \\(H_0(x_0)\\) should be \\(H_{0:T}(x_0)\\) (Feras Saad)\n63, (5.17), top line: the first probability distribution should be with respect to \\(X_t\\), not \\(X_{t-1};\\) i.e. \\(P_t(X_t \\in dx_t | Y_{0:t} = y_{0:t})\\) (Giovanni Diana)\n68, likelihood of future observations in grey box: \\(p(y_{t+2:T}|l)\\) should be \\(p_T(y_{t+2:T}|x_{t+1}=l)\\) (in order to be consistent with notation for LHS) (Yawei Ge).\n73, summary: mathematically equivalently -&gt; mathematically equivalent (Feras Saad)\n76, result 2 (near bottom of the page) \\(y_{1:t}\\) should be \\(y_{0:t}\\) in the first factor of the RHS.\n90, MSE of normalised IS (first displayed eq.): term \\(\\mathbb{Q}(\\varphi)^2\\) should be multiplied by \\(\\mathbb{M}(w)^2\\) (Jacob Hoover).\n92, equation below (8.5): missing sup with respect to \\(\\varphi\\) (Adrien Corenflos)\n92, Theorem 8.5: [w] missing just before big closing parenthesis (Chihiro Kuraya)\n116, second equation in Lemma 9.2: \\(V_n\\) should be \\(V^n\\) (Adrien Corenflos)\n117, equation on last line: in middle expression, n and n - 1 should be superscripts; the intersection inside \\(\\lambda(\\cdot)\\) should be \\([C^{n-1}, C^n] \\cap [\\frac{m-1}{N}, \\frac{m}{N}]\\). (Chihiro Kuraya)\n118: same issue in first multi-line equation: replace \\(n\\) by \\(m\\) in the interval \\([n-1/N, n/N]\\), and below the sum (second line). (Chihiro Kuraya)\n119^4 (display in proof of Prop 9.4): \\(W^n\\) should be \\(W\\). (Chihiro Kuraya)\n119_5: estimate(d) -&gt; estimates (Adrien Corenflos)\n122, Ex 9.10: the first constraint is \\(E(W)=x\\) (missing “=x”). (Omiros)\n126: Guldas et al (2017), “volume 28” -&gt; “volume 48, Issue 28”. (Feras Saad)\n130, Algorithm 10.1: resampling step should be based on weights \\(W_{t-1}^{1:N}\\), not \\(W_t^{1:N}\\) (Chihiro Kuraya)\n131^4: done in the first too (two) lines… (Yawei Ge).\n131, grey box: space missing after “approximates”. (Omiros)\n139, first box, 3rd bullet: “P_t &gt;&gt; M_t”, should be the reverse: “P_t &lt;&lt; M_t”; also, \\(P_t(X_t \\in dx_{0:t} | \\ldots )\\) should be \\(P_t(X_{0:t} \\in dx_{0:t} | \\ldots )\\) (Feras Saad)\n139: the following Radon-Nykodym derive (s) exist (Adrien Corenflos)\n144, last line of Example 10.5: the second term should be \\(\\sigma^2[y_t^2 e^{-\\mu^\\star(x_{t-1})} - 1] /2\\) (Gonzalo Mena)\n152, (10.4): missing tilde on the \\(X_{t-1}\\) in the second term.\n176: Proposition 11.2 should be Proposition 11.5. (Adam Johansen).\n180, snd line of Lemma 11.7: \\(\\phi\\) should be \\(\\varphi\\) (Chihiro Kuraya)\n190: in both lines below the display, h should be k in \\(z_{t-1}\\) (Suzie Brown)\n195, Algorithm 12.1: functions \\(\\Phi_0^n\\) and \\(\\Phi_t^n\\) should read \\(\\Phi_0^N\\) and \\(\\Phi_t^N\\); i.e. we compute recursively \\(\\Phi_t^N(X_t^n)\\). (Mathieu Gerber)\n239: in the 9th line of Alg 13.3, \\(H_{t-1}^{1:N}\\) should be \\(H_t^{1:N}\\) (Chihiro Kuraya)\n258: \\(\\theta^{n-1}\\) should be \\(\\theta_{n-1}\\) in function \\(\\theta\\) -&gt; \\(Q(\\theta, \\theta^{n-1})\\) (Feras Saad)\n264, second displayed equation: last product should be from \\(t=0\\) to \\(T\\) (not \\(t\\))\n267, (14.9): replace \\(\\theta\\) by \\(\\theta_{n-1}\\) in the gradient\n282, first line: give(n) proposal (Adrien Corenflos)\n283, last paragraph: \\(O(d^2)\\) should be \\(O(d_{\\theta}^2)\\) (Feras Saad)\n285, 4th line before Sec 15.5: full conditional should be \\(q_k(\\theta(k)|\\theta(-k))\\) rather than \\(q(\\theta(k)|\\theta(-k))\\) (Chihiro Kuraya)\n287, caption of Fig. 15.2: (t)he first \\(10^5\\) simulations\n295, Example 6.1: better behaved tha(n) (Feras Saad)\n300, Prop 16.1: missing tilde on the \\(M\\) of the proposed kernel (Feras Saad)\n301^1: \\((z/c)\\) instead of \\((Z/c)\\) in the expression of \\(L(\\theta, z)\\) (Chihiro Kuraya)\n332, second paragraph of 17.2.1: covariance matrix should be set to \\(\\lambda^2\\hat{\\Sigma}_{t-1}\\); note the missing square (Mathieu Gerber) In fact, \\(\\lambda\\) should be replaced by \\(\\delta\\) on that page, since that quantity was denoted by \\(\\delta\\) in Chapter 15, and since letter \\(\\lambda\\) means something else in the rest of the chapter (the tempering coefficient).\n337, Algorithm 17.3, second line of the else block: replace \\(\\Theta_t^n\\) by \\(\\Theta_{t-1}^{A_t^n}\\) (Adrien Corenflos)\n342, (17.5): \\(p_t^\\theta\\) should be \\(p_T^\\theta\\)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Nicolas’ blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nNew web-site\n\n\n\n\n\n\n\ntools\n\n\n\n\nQuarto is great\n\n\n\n\n\n\nJun 27, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "particles: Sequential Monte Carlo python library; implements all the methods described in the book. Check the documentation on readthedocs. (See in particular the notebook tutorials)."
  },
  {
    "objectID": "software.html#actively-developed",
    "href": "software.html#actively-developed",
    "title": "Software",
    "section": "",
    "text": "particles: Sequential Monte Carlo python library; implements all the methods described in the book. Check the documentation on readthedocs. (See in particular the notebook tutorials)."
  },
  {
    "objectID": "software.html#older-software",
    "href": "software.html#older-software",
    "title": "Software",
    "section": "Older software",
    "text": "Older software\nSoftware developed by me and/or co-authors for various previous papers:\n\ncabsde (python): Computational aspects of Bayesian spectral density estimation.\npy-smc2 (python): Pierre Jacob’s implementation of SMC^2.\ntruncgauss (python + C): Fast simulation of Truncated Gaussian distribution, see also Vincent Mazet’s Matlab implementation.\nep-abc (matlab): EP-ABC (jointly written with Simon Barthelmé)."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Book",
    "section": "",
    "text": "An introduction to Sequential Monte Carlo\nNicolas Chopin and Omiros Papaspiliopoulos\nAvailable here. See software for the accompanying Python library, particles.\nChapters:\n\nIntroduction\nIntroduction to state-space models\nBeyond state-space models\nIntroduction to Markov processes\nFeynman-Kac models: definition, properties and recursions\nFinite state-spaces and hidden Markov models\nLinear-Gaussian state-space models\nImportance sampling\nImportance resampling\nParticle filtering\nConvergence and stability of particle filters\nParticle smoothing\nSequential quasi-Monte Carlo\nMaximum likelihood estimation of state-space models\nMarkov chain Monte Carlo\nBayesian estimation of state-space models and particle MCMC\nSMC samplers\nSMC^2, sequential inference in state-space models\nAdvanced topics and open problems\n\nTypos: see here."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Publications",
    "section": "",
    "text": "[1]\n\n\nYoussfi, Y., and Chopin, N. Scalable Bayesian bi-level variable selection in generalized linear models. arxiv 2303.12462 (March 2023). [ bib | arXiv ]\n\n\n\n\n[2]\n\n\nSakhi, O., Chopin, N., and Alquier, P. Pac-bayesian offline contextual bandits with guarantees. arxiv 2210.13132 (October 2022). [ bib | DOI | arXiv ]\n\n\n\n\n[3]\n\n\nChopin, N., and Gerber, M. Higher-order stochastic integration through cubic stratification. arxiv 2210.01554 (October 2022). [ bib | DOI | arXiv ]\n\n\n\n\n[4]\n\n\nDau, H.-D., and Chopin, N. On the complexity of backward smoothing algorithms. arxiv 2207.00976 (July 2022). [ bib | DOI | arXiv ]\n\n\n\n\n[5]\n\n\nJin, R., Singh, S. S., and Chopin, N. De-biasing particle filtering for a continuous time hidden markov model with a cox process observation model. arXiv 2206.10478 (2022). [ bib | DOI | arXiv ]\n\n\n\n\n[6]\n\n\nChopin, N., Fulop, A., Heng, J., and Thiery, A. H. Computational doob’s h-transforms for online filtering of discretely observed diffusions. arXiv 2206.03369 (2022). [ bib | DOI | arXiv ]\n\n\n\n\n[7]\n\n\nChopin, N., Singh, S. S., Soto, T., and Vihola, M. On resampling schemes for particle filters with weakly informative observations. Ann. Statist. 50, 6 (2022), 3197–3222. [ bib | DOI | arXiv ]\n\n\n\n\n[8]\n\n\nCorenflos, A., Chopin, N., and Särkkä, S. De-sequentialized monte carlo: a parallel-in-time particle smoother. Journal of Machine Learning Research 23, 283 (2022), 1–39. [ bib | arXiv | .html ]\n\n\n\n\n[9]\n\n\nDucrocq, G., Chopin, N., Errard, J., and Stompor, R. Improved Gibbs samplers for cosmic microwave background power spectrum estimation. Phys. Rev. D 105 (May 2022), 103501. [ bib | DOI | arXiv ]\n\n\n\n\n[10]\n\n\nDau, H.-D., and Chopin, N. Waste-free sequential Monte Carlo. J. R. Stat. Soc. Ser. B. Stat. Methodol. 84, 1 (2022), 114–148. [ bib | DOI ]\n\n\n\n\n[11]\n\n\nChopin, N., and Ducrocq, G. Fast compression of MCMC output. Entropy 23, 8 (2021), Paper No. 1017, 16. [ bib | DOI | arXiv ]\n\n\n\n\n[12]\n\n\nAlvares, D., Armero, C., Forte, A., and Chopin, N. Sequential Monte Carlo methods in Bayesian joint models for longitudinal and time-to-event data. Stat. Model. 21, 1-2 (2021), 161–181. [ bib | DOI ]\n\n\n\n\n[13]\n\n\nFindling, C., Chopin, N., and Koechlin, E. Imprecise neural computations as a source of adaptive behaviour in volatile environments. Nature Human Behaviour (2020), 1–14. [ bib | DOI | arXiv ]\n\n\n\n\n[14]\n\n\nGerber, M., Chopin, N., and Whiteley, N. Negative association, ordering and convergence of resampling methods. Ann. Statist. 47, 4 (2019), 2236–2260. [ bib | DOI | arXiv ]\n\n\n\n\n[15]\n\n\nBuchholz, A., and Chopin, N. Improving Approximate Bayesian Computation via Quasi-Monte Carlo. J. Comput. Graph. Statist. 28, 1 (2019), 205–219. [ bib | DOI | arXiv ]\n\n\n\n\n[16]\n\n\nAndrieu, C., Doucet, A., Yildirim, S., and Chopin, N. On the utility of Metropolis-Hastings with asymmetric acceptance ratio. arXiv e-prints (Mar 2018). [ bib | arXiv ]\n\n\n\n\n[17]\n\n\nRiou-Durand, L., and Chopin, N. Noise contrastive estimation: asymptotic properties, formal comparison with MC-MLE. Electron. J. Stat. 12, 2 (2018), 3473–3518. [ bib | DOI | arXiv ]\n\n\n\n\n[18]\n\n\nChopin, N., and Gerber, M. Sequential quasi–Monte Carlo: introduction for non-experts, dimension reduction, application to partly observed diffusion processes. In Monte Carlo and quasi–Monte Carlo methods, vol. 241 of Springer Proc. Math. Stat. Springer, Cham, 2018, pp. 99–121. [ bib | arXiv ]\n\n\n\n\n[19]\n\n\nAlvares, D., Armero, C., Forte, A., and Chopin, N. Sequential Monte Carlo methods in random intercept models for longitudinal data. In Bayesian statistics in action, vol. 194 of Springer Proc. Math. Stat. Springer, Cham, 2017, pp. 3–9. [ bib | DOI ]\n\n\n\n\n[20]\n\n\nVasishth, S., Chopin, N., Ryder, R., and Nicenboim, B. Modelling dependency completion in sentence comprehension as a Bayesian hierarchical mixture process: A case study involving Chinese relative clauses, July 2017. [ bib | arXiv ]\n\n\n\n\n[21]\n\n\nVasishth, S., Nicenboim, B., Chopin, N., and Ryder, R. Bayesian hierarchical finite mixture models of reading times: A case study. PsyArXiv (July 2017). [ bib | DOI ]\n\n\n\n\n[22]\n\n\nOates, C. J., Girolami, M., and Chopin, N. Control functionals for Monte Carlo integration. J. R. Stat. Soc. Ser. B. Stat. Methodol. 79, 3 (2017), 695–718. [ bib | DOI | arXiv ]\n\n\n\n\n[23]\n\n\nGerber, M., and Chopin, N. Convergence of sequential quasi-Monte Carlo smoothing algorithms. Bernoulli 23, 4B (2017), 2951–2987. [ bib | DOI | arXiv ]\n\n\n\n\n[24]\n\n\nChopin, N., and Ridgway, J. Leave Pima Indians alone: binary regression as a benchmark for Bayesian computation. Statist. Sci. 32, 1 (2017), 64–87. [ bib | DOI | arXiv ]\n\n\n\n\n[25]\n\n\nSchretter, C., He, Z., Gerber, M., Chopin, N., and Niederreiter, H. Van der Corput and golden ratio sequences along the Hilbert space-filling curve. In Monte Carlo and quasi-Monte Carlo methods, vol. 163 of Springer Proc. Math. Stat. Springer, [Cham], 2016, pp. 531–544. [ bib | DOI ]\n\n\n\n\n[26]\n\n\nAlquier, P., Ridgway, J., and Chopin, N. On the properties of variational approximations of Gibbs posteriors. J. Mach. Learn. Res. 17, 239 (2016), Paper No. 239, 41. [ bib | arXiv ]\n\n\n\n\n[27]\n\n\nBarthelmé, S., Chopin, N., and Cottet, V. Divide and conquer in ABC: expectation-propagation algorithms for likelihood-free inference. In Handbook of approximate Bayesian computation, Chapman & Hall/CRC Handb. Mod. Stat. Methods. CRC Press, Boca Raton, FL, 2019, pp. 415–434. [ bib ]\n\n\n\n\n[28]\n\n\nChopin, N., and Gerber, M. Application of sequential Quasi-Monte Carlo to autonomous positioning. In Signal Processing Conference (EUSIPCO), 2015 23rd European (Aug 2015), pp. 489–493. [ bib | DOI | arXiv ]\n\n\n\n\n[29]\n\n\nChopin, N., Ridgway, J., Gerber, M., and Papaspiliopoulos, O. Towards automatic calibration of the number of state particles within the SMC2 algorithm. ArXiv preprint 1506.00570 (Jun 2015). [ bib | arXiv ]\n\n\n\n\n[30]\n\n\nChopin, N., Gadat, S., Guedj, B., Guyader, A., and Vernet, E. On some recent advances on high dimensional Bayesian statistics. In Modélisation Aléatoire et Statistique—Journées MAS 2014, vol. 51 of ESAIM Proc. Surveys. EDP Sci., Les Ulis, 2015, pp. 293–319. [ bib | DOI ]\n\n\n\n\n[31]\n\n\nGerber, M., and Chopin, N. Sequential quasi Monte Carlo. J. R. Stat. Soc. Ser. B. Stat. Methodol. 77, 3 (2015), 509–579. [ bib | DOI | arXiv ]\n\n\n\n\n[32]\n\n\nBarthelmé, S., and Chopin, N. The Poisson transform for unnormalised statistical models. Stat. Comput. 25, 4 (2015), 767–780. [ bib | DOI | arXiv ]\n\n\n\n\n[33]\n\n\nChopin, N., and Singh, S. S. On particle Gibbs sampling. Bernoulli 21, 3 (2015), 1855–1883. [ bib | DOI | arXiv ]\n\n\n\n\n[34]\n\n\nKantas, N., Doucet, A., Singh, S. S., Maciejowski, J., and Chopin, N. On particle methods for parameter estimation in state-space models. Statist. Sci. 30, 3 (2015), 328–351. [ bib | DOI | arXiv ]\n\n\n\n\n[35]\n\n\nGelman, A., Vehtari, A., Jylänki, P., Robert, C., Chopin, N., and Cunningham, J. P. Expectation propagation as a way of life. ArXiv e-prints (dec 2014). [ bib | arXiv ]\n\n\n\n\n[36]\n\n\nRidgway, J., Alquier, P., Chopin, N., and Liang, F. PAC-Bayesian AUC classification and scoring. In Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds. Curran Associates, Inc., 2014, pp. 658–666. [ bib | arXiv ]\n\n\n\n\n[37]\n\n\nAlquier, P., Cottet, V., Chopin, N., and Rousseau, J. Bayesian matrix completion: prior specification and consistency. ArXiv preprint, 1406.1440 (2014). [ bib | arXiv ]\n\n\n\n\n[38]\n\n\nBarthelmé, S., and Chopin, N. Expectation propagation for likelihood-free inference. J. Amer. Statist. Assoc. 109, 505 (2014), 315–333. [ bib | DOI | arXiv ]\n\n\n\n\n[39]\n\n\nAndrieu, C., Chopin, N., Doucet, A., and Rubenthaler, S. Perfect simulation for the Feynman-Kac law on the path space. ArXiv preprint 1210.0376 (Mar 2013). [ bib | arXiv ]\n\n\n\n\n[40]\n\n\nChopin, N., Jacob, P. E., and Papaspiliopoulos, O. SMC2: an efficient algorithm for sequential analysis of state space models. J. R. Stat. Soc. Ser. B. Stat. Methodol. 75, 3 (2013), 397–426. [ bib | DOI | arXiv ]\n\n\n\n\n[41]\n\n\nChopin, N., Rousseau, J., and Liseo, B. Computational aspects of Bayesian spectral density estimation. J. Comput. Graph. Statist. 22, 3 (2013), 533–557. [ bib | DOI | arXiv ]\n\n\n\n\n[42]\n\n\nSingh, S. S., Chopin, N., and Whiteley, N. Bayesian learning of noisy Markov decision processes. ACM Trans. Model. Comput. Simul. 23, 1 (2013), Art. 4, 25. [ bib | DOI | arXiv ]\n\n\n\n\n[43]\n\n\nSchäfer, C., and Chopin, N. Sequential Monte Carlo on large binary sampling spaces. Stat. Comput. 23, 2 (2013), 163–184. [ bib | DOI | arXiv ]\n\n\n\n\n[44]\n\n\nChopin, N., Gelman, A., Mengersen, K. L., and Robert, C. P. In praise of the referee. ArXiv preprint 1205.4304 (May 2012). [ bib | arXiv ]\n\n\n\n\n[45]\n\n\nAndrieu, C., Barthelme, S., Chopin, N., Cornebise, J., Doucet, A., Girolami, M., Kosmidis, I., Jasra, A., Lee, A., Marin, J.-M., Pudlo, P., Robert, C. P., Sedki., M., and Singh, S. S. Some discussions of d. fearnhead and d. prangle’s read paper “constructing summary statistics for approximate bayesian computation: semi-automatic approximate bayesian computation”. ArXiv preprint 1201.1314 (Jan 2012). [ bib | arXiv ]\n\n\n\n\n[46]\n\n\nChopin, N., and Robert, C. Discussion of “catching up faster by switching sooner: a predictive approach to adaptive estimation with an application to the aic–bic dilemma” by Erven, Tim van and Grünwald, Peter and de rooij, Steven. Journal of the Royal Statistical Society (series B) 74, 3 (2012), 361–417. [ bib ]\n\n\n\n\n[47]\n\n\nRousseau, J., Chopin, N., and Liseo, B. Bayesian nonparametric estimation of the spectral density of a long or intermediate memory Gaussian process. Ann. Statist. 40, 2 (2012), 964–995. [ bib | DOI | arXiv ]\n\n\n\n\n[48]\n\n\nChopin, N., Lelièvre, T., and Stoltz, G. Free energy methods for Bayesian inference: efficient exploration of univariate Gaussian mixture posteriors. Stat. Comput. 22, 4 (2012), 897–916. [ bib | DOI | arXiv ]\n\n\n\n\n[49]\n\n\nBarthelmé, S., Beffy, M., Chopin, N., Doucet, A., Jacob, P., Johansen, A., Marin, J., and C.P., R. Discussions on “Riemann manifold langevin and Hamiltonian Monte Carlo methods” by M. Girolami and B. Caldherhead. Journal of the Royal Statistical Society (series B) 73, 2 (2011), 123–214. [ bib | DOI | arXiv ]\n\n\n\n\n[50]\n\n\nChopin, N., and Robert, C. Comments on “Using TPA for Bayesian inference” by Huber, m. and Schott, s. In Bayesian Statistics 9 (2011), J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, S. A. F. M., and M. West, Eds., Oxford University Press, pp. 257–282. [ bib ]\n\n\n\n\n[51]\n\n\nChopin, N., and Papaspiliopoulos, O. Comments on “Bayesian variable selection for random intercept modeling of Gaussian and non-Gaussian data” by Frühwirth-schnatter, s. and Wagner, h. In Bayesian Statistics 9 (2011), J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, S. A. F. M., and M. West, Eds., Oxford University Press, pp. 165–200. [ bib ]\n\n\n\n\n[52]\n\n\nChopin, N. Fast simulation of truncated Gaussian distributions. Stat. Comput. 21, 2 (2011), 275–288. [ bib | DOI | arXiv ]\n\n\n\n\n[53]\n\n\nChopin, N., Del Moral, P., and Rubenthaler, S. Stability of Feynman-Kac formulae with path-dependent potentials. Stochastic Process. Appl. 121, 1 (2011), 38–60. [ bib | DOI | arXiv ]\n\n\n\n\n[54]\n\n\nChopin, N., Iacobucci, A., Marin, J., Mengersen, K., Robert, C., Ryder, R., and Schfer, C. On particle learning; comments on “particle learning for sequential bayesian computation” by lopes, carvalho, johannes, and polson. In Bayesian Statistics 9 (2011), J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, S. A. F. M., and M. West, Eds., Oxford University Press, pp. 317–360. [ bib | arXiv ]\n\n\n\n\n[55]\n\n\nChopin, N., and Jacob, P. Free energy sequential Monte Carlo, application to mixture modelling. In Bayesian statistics 9, J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, S. A. F. M., and M. West, Eds. Oxford Univ. Press, Oxford, 2011, pp. 91–118. With discussions by Peter J. Green and Benjamin M. Taylor. [ bib | DOI | arXiv ]\n\n\n\n\n[56]\n\n\nBarthelmé, S., and Chopin, N. Abc-ep: Expectation propagation for likelihood-free bayesian computation. In Proceedings of the 28th International Conference on Machine Learning (ICML-11) (New York, NY, USA, June 2011), L. Getoor and T. Scheffer, Eds., ICML ’11, ACM, pp. 289–296. [ bib ]\n\n\n\n\n[57]\n\n\nRobert, C. P., Chopin, N., and Rousseau, J. Rejoinder: Harold Jeffreys’s Theory of probability revisited. ArXiv preprint 0909.1008 (Jan 2010). [ bib | arXiv ]\n\n\n\n\n[58]\n\n\nChopin, N., and Robert, C. P. Properties of nested sampling. Biometrika 97, 3 (2010), 741–755. [ bib | DOI | arXiv ]\n\n\n\n\n[59]\n\n\nJacob, P., Chopin, N., Robert, C. P., and Rue, H. Comments on “Particle Markov chain Monte Carlo” by C. Andrieu, A. Doucet, and R. Hollenstein. ArXiv preprint 0911.0985 (Nov 2009). [ bib | DOI | arXiv ]\n\n\n\n\n[60]\n\n\nRue, H., Martino, S., and Chopin, N. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. J. R. Stat. Soc. Ser. B Stat. Methodol. 71, 2 (2009), 319–392. [ bib | DOI ]\n\n\n\n\n[61]\n\n\nRobert, C. P., Chopin, N., and Rousseau, J. Harold Jeffreys’s theory of probability revisited. Statist. Sci. 24, 2 (2009), 141–172. [ bib | DOI | arXiv ]\n\n\n\n\n[62]\n\n\nChopin, N. Jim Albert: Bayesian computation with R. Statistics and Computing 19 (2009), 111–112. [ bib | DOI ]\n\n\n\n\n[63]\n\n\nChopin, N. On the equivalence between standard and sequentially ordered hidden Markov models. Statist. Probab. Lett. 78, 14 (2008), 2171–2174. [ bib | DOI | arXiv ]\n\n\n\n\n[64]\n\n\nChopin, N., and Robert, C. Comment on `Nested sampling’ by Skilling. In Bayesian Statistics 8 (2007), O. U. P. Bernardo, J. M. et al. (eds), Ed., pp. 491–524. [ bib ]\n\n\n\n\n[65]\n\n\nChopin, N., and Robert, C. Comment on ‘Estimating the integrated likelihood via posterior simulation using the harmonic mean equality’, by Raftery et al. In Bayesian Statistics 8 (2007), J. M. e. a. e. Bernardo, Ed., Oxford University Press, pp. 371–416. [ bib ]\n\n\n\n\n[66]\n\n\nChopin, N., and Fearnhead, P. Comment on `objective bayesian analysis of multiple changepoints for linear models’, by Giron et al. In Bayesian Statistics 8 (2007), O. U. P. Bernardo, J. M. et al. (eds), Ed., pp. 227–252. [ bib ]\n\n\n\n\n[67]\n\n\nChopin, N. Comment on `sequential Monte Carlo for Bayesian computation’ by Del Moral et al. In Bayesian Statistics 8 (2007), O. U. P. Bernardo, J. M. et al. (eds), Ed., pp. 115–148. [ bib ]\n\n\n\n\n[68]\n\n\nRue, H., Martino, S., and Chopin, N. Discussion on `Modern statistics for spatial point processes’ by Mller and Waagepetersen. Scandinavian Journal of Statistics 34, 4 (2007), 685–711. [ bib ]\n\n\n\n\n[69]\n\n\nChopin, N. Dynamic detection of change points in long time series. Ann. Inst. Statist. Math. 59, 2 (2007), 349–366. [ bib | DOI ]\n\n\n\n\n[70]\n\n\nChopin, N. Inference and model choice for sequentially ordered hidden Markov models. J. R. Stat. Soc. Ser. B Stat. Methodol. 69, 2 (2007), 269–284. [ bib | DOI ]\n\n\n\n\n[71]\n\n\nChopin, N., and Varini, E. Particle filtering for continuous-time hidden Markov models. In Conference Oxford sur les méthodes de Monte Carlo séquentielles, vol. 19 of ESAIM Proc. EDP Sci., Les Ulis, 2007, pp. 12–17. [ bib | DOI ]\n\n\n\n\n[72]\n\n\nChopin, N. Discussion of `exact and efficient likelihood-based estimation for discretely observed diffusion processes’ by Beskos et al. Journal of the Royal Statistical Society (series B), 68 (2006), 333–382. [ bib ]\n\n\n\n\n[73]\n\n\nChopin, N. Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference. Ann. Statist. 32, 6 (2004), 2385–2411. [ bib | DOI | arXiv ]\n\n\n\n\n[74]\n\n\nChopin, N., and Pelgrin, F. Bayesian inference and state number determination for hidden Markov models: an application to the information content of the yield curve about inflation. J. Econometrics 123, 2 (2004), 327–344. [ bib | DOI ]\n\n\n\n\n[75]\n\n\nComment on `iid sampling with self-avoiding particle filters: the pinball sampler by Mengersen and Robert. In Bayesian Statistics 7 (2003), O. U. P. Bernardo, J. M. et al. (eds), Ed., pp. 277–292. [ bib ]\n\n\n\n\n[76]\n\n\nChopin, N. A sequential particle filter method for static models. Biometrika 89, 3 (2002), 539–551. [ bib | DOI ]\n\n\n\n\n\n\nThis file was generated by bibtex2html 1.99."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Introduction aux processus: poly\nsimulation and Monte Carlo methods: slides\nState-space models and sequential Monte Carlo methods: slides"
  },
  {
    "objectID": "teaching.html#ensae",
    "href": "teaching.html#ensae",
    "title": "Teaching",
    "section": "",
    "text": "Introduction aux processus: poly\nsimulation and Monte Carlo methods: slides\nState-space models and sequential Monte Carlo methods: slides"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas Chopin",
    "section": "",
    "text": "I am a Professor of Data Sciences/Statistics/Machine Learning at ENSAE, Institut Polytechnique de Paris.\nI am interested in all aspects of Bayesian computation, that is algorithms to perform Bayesian inference, including:\n\nMonte Carlo methods: particularly Sequential Monte Carlo, but also plain, quasi- and Markov chain Monte Carlo;\nfast approximations: e.g. Expectation Propagation and variational Bayes.\n\nI wrote a book on Sequential Monte Carlo, see here; for the accompanying python library, see here."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Nicolas Chopin",
    "section": "",
    "text": "I am a Professor of Data Sciences/Statistics/Machine Learning at ENSAE, Institut Polytechnique de Paris.\nI am interested in all aspects of Bayesian computation, that is algorithms to perform Bayesian inference, including:\n\nMonte Carlo methods: particularly Sequential Monte Carlo, but also plain, quasi- and Markov chain Monte Carlo;\nfast approximations: e.g. Expectation Propagation and variational Bayes.\n\nI wrote a book on Sequential Monte Carlo, see here; for the accompanying python library, see here."
  },
  {
    "objectID": "index.html#editorial-responsibilities",
    "href": "index.html#editorial-responsibilities",
    "title": "Nicolas Chopin",
    "section": "Editorial responsibilities",
    "text": "Editorial responsibilities\nI am currently an Associate editor for:\n\nAnnals of Statistics\nBiometrika\n\nI rarely find the time to review extra papers for other journals, sorry."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]